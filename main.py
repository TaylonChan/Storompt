from langchain.llms import CTransformers
from langchain import LLMChain
from langchain.prompts import PromptTemplate, load_prompt
from langchain.prompts.pipeline import PipelinePromptTemplate
from langchain.prompts.chat import (
    SystemMessage, 
    ChatPromptTemplate,
    HumanMessagePromptTemplate
)
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

from transformers import pipeline

import gradio as gr
import json

# Debug Libs
import langchain
import time

# langchain.debug = True

# FAMPE : A Composite Affective Model Based on Favorability, Attitude, Mood, Personality and Emotion

MODEL_PATH = "./src/models/Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_K_M.bin"

charactor_name = "lila_nightshade"
char_human = "Boy"
char_aibot = json.load(open(f'./charactor/{charactor_name}.json'))

suggestion = json.dumps("")

distilled_student_sentiment_classifier = pipeline(
    model="lxyuan/distilbert-base-multilingual-cased-sentiments-student", 
    return_all_scores=True
)

sentiment_result = distilled_student_sentiment_classifier("Eirax Darkhunter is a ruggedly handsome man with jet black hair and piercing blue eyes. He wears a worn leather armor and carries a massive greatsword at his side. Despite his fearsome appearance, Eirax has a kind heart and will stop at nothing to protect those he considers friends. His primary motivation is to rid the world of evil forces threatening civilization, and his biggest fear is losing loved ones to darkness.")
print(sentiment_result)

def create_prompt() -> PipelinePromptTemplate:
    """Creates prompt template"""
    charactor_prompt = PromptTemplate.from_template(char_aibot["template"])
    conrecord_prompt = PromptTemplate.from_template(
    """
Current conversation:
{history}
{human}: {input}
{aibot}:""",
        partial_variables={
            "human": char_human,
            "aibot": char_aibot["name"]
        }
    )
    example_prompt = PromptTemplate.from_template(char_aibot["examples"])

    template = """
I want you to act as the following charactor in first person narrative with emotion and action. Please make a verbose response to extend the plot.
{charactor}

Example:
{example}

{conrecord}"""

    final_prompt = PromptTemplate.from_template(template)
    input_prompts = [
        ("charactor", charactor_prompt),
        ("example", example_prompt),
        ("conrecord", conrecord_prompt)
    ]
    pipeline_prompt = PipelinePromptTemplate(
        final_prompt=final_prompt, 
        pipeline_prompts=input_prompts, 
        input_variables=["history", "input"]
    )

    return pipeline_prompt

def create_suggestion_prompt() -> PipelinePromptTemplate:
    """Creates suggestion prompt template"""
    charactor_prompt = PromptTemplate.from_template(char_aibot["template"])
    conrecord_prompt = PromptTemplate.from_template(
    """
Current conversation:
{history} {input}"""
    )

    template = """
I want you to act as the following charactor in first person narrative with emotion and action. Please make a verbose response to extend the plot.

{conrecord}
Boy:"""

    final_prompt = PromptTemplate.from_template(template)
    input_prompts = [
        ("conrecord", conrecord_prompt)
    ]
    pipeline_prompt = PipelinePromptTemplate(
        final_prompt=final_prompt, 
        pipeline_prompts=input_prompts,
        input_variables=["history", "input"]
    )
    return pipeline_prompt

def load_model(model_path, stop=['\n']) -> CTransformers:
    """Load Llama model with defined setting"""
    callback_manager: CallbackManager = CallbackManager([StreamingStdOutCallbackHandler()])
    n_gpu_layers = 40
    n_batch = 512

    # make sure the model path is correct for your system!
    llama_model: CTransformers = CTransformers(
        model=model_path, 
        model_type='llama', 
        verbose=True,
        temperature=0.85,
        max_new_token=2048,
        n_gpu_layers=n_gpu_layers,
        n_batch=n_batch,
        callback_manager=callback_manager,
        config={
            'context_length': 2048,
            'stream': True,
            'stop': stop
        }
    )
    
    return llama_model

def make_response(message, history):
    memory = ConversationBufferMemory(ai_prefix=char_aibot["name"], human_prefix=char_human)
    for user_msg, ai_msg in history:
        memory.chat_memory.add_user_message(user_msg)
        memory.chat_memory.add_ai_message(ai_msg)

    while True:
        conversation = ConversationChain(
            prompt=prompt,
            llm=llm, 
            memory=memory,
            verbose=True
        )

        response = conversation.predict(input=message)
        if not response or response.isspace():
            print("Empty Response")
        else:
            break

    history.append((message, response))
    return "", history

def make_suggestion(message, history):
    memory = ConversationBufferMemory(ai_prefix=char_aibot["name"], human_prefix=char_human)
    for user_msg, ai_msg in history:
        memory.chat_memory.add_user_message(user_msg)
        memory.chat_memory.add_ai_message(ai_msg)

    while True:
        suggestion_result = ConversationChain(
            prompt=prompt_suggest,
            llm=llm, 
            memory=memory,
            verbose=True
        )

        response = suggestion_result.predict(input=message)
        if not response or response.isspace():
            print("Empty Response")
        else:
            break

    suggestion = json.dumps(response)
    return suggestion

def change_charactor(charactor):
    global charactor_name, char_aibot
    charactor_name = charactor
    char_aibot = json.load(open(f'./charactor/{charactor_name}.json'))
    init_chat()

def init_chat():
    global llm, prompt, prompt_suggest
    llm = load_model(MODEL_PATH)
    prompt = create_prompt()
    prompt_suggest = create_suggestion_prompt()

init_chat()

with gr.Blocks() as app:
    with gr.Row():
        with gr.Column():
            radio_char = gr.Radio(
                [
                    ("Elara Moonwhisper", "elara_moonwhisper"),
                    ("Lila Nightshade", "lila_nightshade"),
                    ("Eirax Darkhunter", "eirax_darkhunter"),
                    ("Aurora Nightshade", "aurora_nightshade"),
                    ("Arcturus Starseeker", "arcturus_starseeker"),
                    ("Arcturus Ironwood", "arcturus_ironwood")
                ],
                label="Please choose a charactor",
                show_label=True
            )
            gr.JSON(
                {
                    "favorability": 0,
                },
                label="Emotion",
                show_label=True
            )
            com_suggestion = gr.JSON(
                suggestion,
                label="Suggestion",
                show_label=True
            )
        with gr.Column():
            chatbot = gr.Chatbot(
                bubble_full_width=False,
                show_share_button=True,
                show_copy_button=True,
                likeable=True
            )
            message = gr.Textbox(
                label="Your Response"
            )
            with gr.Row():
                submit = gr.Button(
                    value="‚ú® Deliver",
                    scale=3
                )
                clear = gr.ClearButton(
                    [message, chatbot],
                    value="üóëÔ∏è Clear"
                )
            gr.on(
                [message.submit, submit.click],
                make_response, 
                [message, chatbot], 
                [message, chatbot]
            ).then(
                make_suggestion, 
                [message, chatbot], 
                com_suggestion
            )
            radio_char.change(
                change_charactor,
                radio_char,
            ).then(
                None,
                js="window.location.reload()"
            )

app.launch(
    ## share=True,
    inbrowser=True,
    show_error=True
)